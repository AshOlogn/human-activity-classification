{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data was given in the form of text files, but I wrote a script to tabulate it in a csv file that can easily be read into a DataFrame. That boilerplate code is omitted, but the csv files themselves clearly denote the features provided. In the following walkthrough, `data` is the training data (itself split for cross-validation until the very end), `test` is the test data (not used at all in model development), `labels` is the training data classes, and `answers` is the test data classes.\n",
    "\n",
    "Also, this walkthrough does NOT use any of the time-series data provided, only the discrete feature vectors. As you will see, we achieve a respectable accuracy even with this subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "data.pop('Unnamed: 0')\n",
    "test.pop('Unnamed: 0')\n",
    "\n",
    "labels = data.pop('class')\n",
    "answers = test.pop('class')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before testing different models, we create a helper method that automatically cross-validates a model given the data, model itself, and number of folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "#first we set up a training/scoring function that will be used in the future also\n",
    "#it uses K-fold cross-validation to more accurately estimate training error\n",
    "def kfold_test(k = 10, model = AdaBoostClassifier(), data = None, labels = None):\n",
    "    \n",
    "    kf = KFold(n_splits = k)\n",
    "    score = []\n",
    "    \n",
    "    for train,test in kf.split(data):\n",
    "\n",
    "        #get labels\n",
    "        train_y = labels[train]\n",
    "        test_y = labels[test]\n",
    "\n",
    "        #train with provided model\n",
    "        model.fit(data.iloc[train], train_y)\n",
    "\n",
    "        #calculate score\n",
    "        score.append(model.score(data.iloc[test], test_y))\n",
    "    \n",
    "    average_error = sum(score)*1.0/len(score)\n",
    "    return average_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try a _really_ naive approach: just use basic algorithms on all 561 features. It will probably fail miserably, but we'll at least get a baseline, and a feel for which base classifiers we should even attempt to tune. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8989407719609582"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 20)\n",
    "kfold_test(10, knn, data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9412420511682933"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logr = LogisticRegression()\n",
    "kfold_test(10, logr, data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6897376885536823"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "kfold_test(10, nb, data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8970354554865423"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='linear')\n",
    "kfold_test(10, svc, data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8446657793552204\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "kfold_test(10, dt, data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5467912599822539"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada = AdaBoostClassifier()\n",
    "kfold_test(10, ada, data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8962204229517894\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "kfold_test(10, rf, data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.922604074238391"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "nn = MLPClassifier()\n",
    "kfold_test(10, nn, data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a decent set of baselines, the next step should be to start pruning features to see how that affects the accuracies of the above naive models. The first step should be to get rid of all features that have very low variance, those that don't change enough across samples to be useful in training. We also choose to abandon AdaBoost since it performs so poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367 fBodyAccJerk-entropy()-X                       36918.10\n",
      "41 tGravityAcc-mean()-X                            29362.64\n",
      "53 tGravityAcc-min()-X                             28175.40\n",
      "368 fBodyAccJerk-entropy()-Y                       28115.92\n",
      "50 tGravityAcc-max()-X                             26686.80\n",
      "57 tGravityAcc-energy()-X                          25981.56\n",
      "524 fBodyBodyAccJerkMag-entropy()                  23619.25\n",
      "235 tBodyAccJerkMag-entropy()                      23437.11\n",
      "288 fBodyAcc-entropy()-X                           23221.61\n",
      "103 tBodyAccJerk-entropy()-X                       22267.09\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "k = 100\n",
    "select = SelectKBest(k=k)\n",
    "select.fit(data, labels)\n",
    "\n",
    "ranked = list(zip(data.columns, select.scores_))\n",
    "ranked.sort(key = itemgetter(1))\n",
    "ranked.reverse()\n",
    "\n",
    "for (ranked, score) in ranked[:10]:\n",
    "    print(\"%-50s %.2f\" % (ranked, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn: 0.8756848935226266\n",
      "logr: 0.9071081041112097\n",
      "nb: 0.765232179828453\n",
      "svc: 0.8769114167406092\n",
      "dt: 0.8139250591540964\n",
      "rf: 0.8554161120970127\n",
      "mlp: 0.8835775288376219\n"
     ]
    }
   ],
   "source": [
    "data_k = pd.DataFrame(select.transform(data))\n",
    "models = [('knn', KNeighborsClassifier(n_neighbors=20)), ('logr', LogisticRegression()), ('nb', GaussianNB()),\n",
    "         ('svc', SVC(kernel='linear')), ('dt', DecisionTreeClassifier()), ('rf', RandomForestClassifier()), \n",
    "         ('mlp', MLPClassifier())]\n",
    "\n",
    "for name, model in models:\n",
    "    print(name + \": \" + str(kfold_test(10, model, data_k, labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems that selecting just the top K features increases error slightly, but that at least tells us that a fraction of the features are responsible for most of the learning that takes place. The next step is to weed out correlated features and try again. We borrowed code from https://stackabuse.com/applying-filter-methods-in-python-for-feature-selection/ to save time. We remove feature pairs with at least 0.8 correlation. This should make the feature set less cluttered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of correlated features: 416\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1 tBodyAcc-mean()-X</th>\n",
       "      <th>2 tBodyAcc-mean()-Y</th>\n",
       "      <th>3 tBodyAcc-mean()-Z</th>\n",
       "      <th>4 tBodyAcc-std()-X</th>\n",
       "      <th>26 tBodyAcc-arCoeff()-X,1</th>\n",
       "      <th>29 tBodyAcc-arCoeff()-X,4</th>\n",
       "      <th>30 tBodyAcc-arCoeff()-Y,1</th>\n",
       "      <th>32 tBodyAcc-arCoeff()-Y,3</th>\n",
       "      <th>33 tBodyAcc-arCoeff()-Y,4</th>\n",
       "      <th>34 tBodyAcc-arCoeff()-Z,1</th>\n",
       "      <th>...</th>\n",
       "      <th>533 fBodyBodyGyroMag-min()</th>\n",
       "      <th>538 fBodyBodyGyroMag-maxInds</th>\n",
       "      <th>540 fBodyBodyGyroMag-skewness()</th>\n",
       "      <th>551 fBodyBodyGyroJerkMag-maxInds</th>\n",
       "      <th>552 fBodyBodyGyroJerkMag-meanFreq()</th>\n",
       "      <th>553 fBodyBodyGyroJerkMag-skewness()</th>\n",
       "      <th>555 angle(tBodyAccMean,gravity)</th>\n",
       "      <th>556 angle(tBodyAccJerkMean),gravityMean)</th>\n",
       "      <th>557 angle(tBodyGyroMean,gravityMean)</th>\n",
       "      <th>558 angle(tBodyGyroJerkMean,gravityMean)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.288585</td>\n",
       "      <td>-0.020294</td>\n",
       "      <td>-0.132905</td>\n",
       "      <td>-0.995279</td>\n",
       "      <td>0.929294</td>\n",
       "      <td>-0.058526</td>\n",
       "      <td>0.256892</td>\n",
       "      <td>0.264106</td>\n",
       "      <td>-0.095246</td>\n",
       "      <td>0.278851</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.989498</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.586156</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.074323</td>\n",
       "      <td>-0.298676</td>\n",
       "      <td>-0.112754</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>-0.464761</td>\n",
       "      <td>-0.018446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.278419</td>\n",
       "      <td>-0.016411</td>\n",
       "      <td>-0.123520</td>\n",
       "      <td>-0.998245</td>\n",
       "      <td>0.611627</td>\n",
       "      <td>0.284595</td>\n",
       "      <td>0.115705</td>\n",
       "      <td>0.294310</td>\n",
       "      <td>-0.281211</td>\n",
       "      <td>0.085988</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.991829</td>\n",
       "      <td>-0.948718</td>\n",
       "      <td>-0.336310</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.158075</td>\n",
       "      <td>-0.595051</td>\n",
       "      <td>0.053477</td>\n",
       "      <td>-0.007435</td>\n",
       "      <td>-0.732626</td>\n",
       "      <td>0.703511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.279653</td>\n",
       "      <td>-0.019467</td>\n",
       "      <td>-0.113462</td>\n",
       "      <td>-0.995380</td>\n",
       "      <td>0.273025</td>\n",
       "      <td>-0.164739</td>\n",
       "      <td>0.017150</td>\n",
       "      <td>0.342256</td>\n",
       "      <td>-0.332564</td>\n",
       "      <td>0.239281</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.995703</td>\n",
       "      <td>-0.794872</td>\n",
       "      <td>-0.535352</td>\n",
       "      <td>-0.555556</td>\n",
       "      <td>0.414503</td>\n",
       "      <td>-0.390748</td>\n",
       "      <td>-0.118559</td>\n",
       "      <td>0.177899</td>\n",
       "      <td>0.100699</td>\n",
       "      <td>0.808529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.279174</td>\n",
       "      <td>-0.026201</td>\n",
       "      <td>-0.123283</td>\n",
       "      <td>-0.996091</td>\n",
       "      <td>0.061436</td>\n",
       "      <td>-0.264307</td>\n",
       "      <td>0.072545</td>\n",
       "      <td>0.323154</td>\n",
       "      <td>-0.170813</td>\n",
       "      <td>0.294938</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.996199</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.230091</td>\n",
       "      <td>-0.936508</td>\n",
       "      <td>0.404573</td>\n",
       "      <td>-0.117290</td>\n",
       "      <td>-0.036788</td>\n",
       "      <td>-0.012892</td>\n",
       "      <td>0.640011</td>\n",
       "      <td>-0.485366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.276629</td>\n",
       "      <td>-0.016570</td>\n",
       "      <td>-0.115362</td>\n",
       "      <td>-0.998139</td>\n",
       "      <td>0.313276</td>\n",
       "      <td>0.086904</td>\n",
       "      <td>0.257615</td>\n",
       "      <td>0.434728</td>\n",
       "      <td>-0.315375</td>\n",
       "      <td>0.439744</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.998353</td>\n",
       "      <td>-0.897436</td>\n",
       "      <td>-0.510282</td>\n",
       "      <td>-0.936508</td>\n",
       "      <td>0.087753</td>\n",
       "      <td>-0.351471</td>\n",
       "      <td>0.123320</td>\n",
       "      <td>0.122542</td>\n",
       "      <td>0.693578</td>\n",
       "      <td>-0.615971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 145 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1 tBodyAcc-mean()-X  2 tBodyAcc-mean()-Y  3 tBodyAcc-mean()-Z  \\\n",
       "0             0.288585            -0.020294            -0.132905   \n",
       "1             0.278419            -0.016411            -0.123520   \n",
       "2             0.279653            -0.019467            -0.113462   \n",
       "3             0.279174            -0.026201            -0.123283   \n",
       "4             0.276629            -0.016570            -0.115362   \n",
       "\n",
       "   4 tBodyAcc-std()-X  26 tBodyAcc-arCoeff()-X,1  29 tBodyAcc-arCoeff()-X,4  \\\n",
       "0           -0.995279                   0.929294                  -0.058526   \n",
       "1           -0.998245                   0.611627                   0.284595   \n",
       "2           -0.995380                   0.273025                  -0.164739   \n",
       "3           -0.996091                   0.061436                  -0.264307   \n",
       "4           -0.998139                   0.313276                   0.086904   \n",
       "\n",
       "   30 tBodyAcc-arCoeff()-Y,1  32 tBodyAcc-arCoeff()-Y,3  \\\n",
       "0                   0.256892                   0.264106   \n",
       "1                   0.115705                   0.294310   \n",
       "2                   0.017150                   0.342256   \n",
       "3                   0.072545                   0.323154   \n",
       "4                   0.257615                   0.434728   \n",
       "\n",
       "   33 tBodyAcc-arCoeff()-Y,4  34 tBodyAcc-arCoeff()-Z,1  \\\n",
       "0                  -0.095246                   0.278851   \n",
       "1                  -0.281211                   0.085988   \n",
       "2                  -0.332564                   0.239281   \n",
       "3                  -0.170813                   0.294938   \n",
       "4                  -0.315375                   0.439744   \n",
       "\n",
       "                     ...                     533 fBodyBodyGyroMag-min()  \\\n",
       "0                    ...                                      -0.989498   \n",
       "1                    ...                                      -0.991829   \n",
       "2                    ...                                      -0.995703   \n",
       "3                    ...                                      -0.996199   \n",
       "4                    ...                                      -0.998353   \n",
       "\n",
       "   538 fBodyBodyGyroMag-maxInds  540 fBodyBodyGyroMag-skewness()  \\\n",
       "0                     -1.000000                         0.586156   \n",
       "1                     -0.948718                        -0.336310   \n",
       "2                     -0.794872                        -0.535352   \n",
       "3                     -1.000000                        -0.230091   \n",
       "4                     -0.897436                        -0.510282   \n",
       "\n",
       "   551 fBodyBodyGyroJerkMag-maxInds  552 fBodyBodyGyroJerkMag-meanFreq()  \\\n",
       "0                         -1.000000                            -0.074323   \n",
       "1                         -1.000000                             0.158075   \n",
       "2                         -0.555556                             0.414503   \n",
       "3                         -0.936508                             0.404573   \n",
       "4                         -0.936508                             0.087753   \n",
       "\n",
       "   553 fBodyBodyGyroJerkMag-skewness()  555 angle(tBodyAccMean,gravity)  \\\n",
       "0                            -0.298676                        -0.112754   \n",
       "1                            -0.595051                         0.053477   \n",
       "2                            -0.390748                        -0.118559   \n",
       "3                            -0.117290                        -0.036788   \n",
       "4                            -0.351471                         0.123320   \n",
       "\n",
       "   556 angle(tBodyAccJerkMean),gravityMean)  \\\n",
       "0                                  0.030400   \n",
       "1                                 -0.007435   \n",
       "2                                  0.177899   \n",
       "3                                 -0.012892   \n",
       "4                                  0.122542   \n",
       "\n",
       "   557 angle(tBodyGyroMean,gravityMean)  \\\n",
       "0                             -0.464761   \n",
       "1                             -0.732626   \n",
       "2                              0.100699   \n",
       "3                              0.640011   \n",
       "4                              0.693578   \n",
       "\n",
       "   558 angle(tBodyGyroJerkMean,gravityMean)  \n",
       "0                                 -0.018446  \n",
       "1                                  0.703511  \n",
       "2                                  0.808529  \n",
       "3                                 -0.485366  \n",
       "4                                 -0.615971  \n",
       "\n",
       "[5 rows x 145 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlated_features = set()  \n",
    "correlation_matrix = data.corr()\n",
    "\n",
    "for i in range(len(correlation_matrix .columns)):  \n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "\n",
    "print('number of correlated features: ' + str(len(correlated_features)))\n",
    "data_pruned = data.drop(labels=correlated_features, axis=1)\n",
    "data_pruned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "416/561 features seem to be correlated with each other, a significant number. We remove all of those features and rerun the list of models we created earlier to see how accuracy is affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn: 0.8669763013901213\n",
      "logr: 0.9283176205264715\n",
      "nb: 0.82997892635315\n",
      "svc: 0.9179813664596272\n",
      "dt: 0.8411305826678499\n",
      "rf: 0.888879769299024\n",
      "mlp: 0.9224682046731736\n"
     ]
    }
   ],
   "source": [
    "for name, model in models:\n",
    "    print(name + \": \" + str(kfold_test(10, model, data_pruned, labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results show a marked improvement in comparison to K-best and are similar to those obtained with all the features. Thus, we conclude that our correlation matrix strategy does indeed isolate the subset of features that are most important to learn. From now on we focus on tuning the following models since they're the most promising:\n",
    "* Logistic regression\n",
    "* Support vector machine\n",
    "* Multilayer perceptron\n",
    "\n",
    "Since we're training fewer models, we can even iterate through many values of k and graph the accuracies of each model for each value of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished k = 10\n",
      "finished k = 20\n",
      "finished k = 30\n",
      "finished k = 40\n",
      "finished k = 50\n",
      "finished k = 60\n",
      "finished k = 70\n",
      "finished k = 80\n",
      "finished k = 90\n",
      "finished k = 100\n"
     ]
    }
   ],
   "source": [
    "logr_acc = []\n",
    "svc_acc = []\n",
    "mlp_acc = []\n",
    "\n",
    "#try values of k = 10,20,30,...,100\n",
    "for k in range(10,110,10):\n",
    "    select = SelectKBest(k=k)\n",
    "    select.fit(data_pruned, labels)\n",
    "    \n",
    "    #get rid of all but the top k features\n",
    "    data_k = pd.DataFrame(select.transform(data_pruned))\n",
    "    logr_acc.append(kfold_test(10, LogisticRegression(), data_k, labels))\n",
    "    svc_acc.append(kfold_test(10, SVC(kernel='linear'), data_k, labels))\n",
    "    mlp_acc.append(kfold_test(10, MLPClassifier(), data_k, labels))\n",
    "    print('finished k = ' + str(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe3a93027b8>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "k_col = []\n",
    "model_col = []\n",
    "accuracy_col = []\n",
    "\n",
    "for i in range(10):\n",
    "    k_col.append(10*(i+1))\n",
    "    model_col.append('Logistic')\n",
    "    accuracy_col.append(logr_acc[i])\n",
    "    \n",
    "    k_col.append(10*(i+1))\n",
    "    model_col.append('SVC')\n",
    "    accuracy_col.append(svc_acc[i])\n",
    "    \n",
    "    k_col.append(10*(i+1))\n",
    "    model_col.append('MLP')\n",
    "    accuracy_col.append(mlp_acc[i])\n",
    "\n",
    "graph_data = pd.DataFrame()\n",
    "graph_data['K'] = k_col\n",
    "graph_data['Accuracy'] = accuracy_col\n",
    "graph_data['Model'] = model_col\n",
    "sns.scatterplot(x='K', y='Accuracy', hue='Model', data=graph_data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph, we can say that k = 80 is a safe choice. Thus, we have successfully pruned our feature set from 561 to 80. RFECV ended up increasing error, so that trial is omitted here. All 3 selected models perform very well with 80 features, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_80 shape: (7352, 80)\n"
     ]
    }
   ],
   "source": [
    "#first save the data using top 80 features (all other features discarded from now on)\n",
    "select = SelectKBest(k=80)\n",
    "select = select.fit(data_pruned, labels)\n",
    "\n",
    "ranked = list(zip(data_pruned.columns, select.scores_))\n",
    "ranked.sort(key = itemgetter(1))\n",
    "ranked.reverse()\n",
    "\n",
    "data_80 = pd.DataFrame()\n",
    "for (feature, score) in ranked[:80]:\n",
    "    data_80[feature] = data_pruned[feature] \n",
    "\n",
    "print('data_80 shape: ' + str(data_80.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final data: (7352, 79)\n",
      "logr: 0.9230157497781721\n",
      "svc: 0.9213829118603964\n",
      "mlp: 0.9160760499852113\n"
     ]
    }
   ],
   "source": [
    "print('final data: ' + str(data_final.shape))\n",
    "print('logr: ' + str(kfold_test(10, LogisticRegression(), data_80, labels)))\n",
    "print('svc: ' + str(kfold_test(10, SVC(kernel='linear'), data_80, labels)))\n",
    "print('mlp: ' + str(kfold_test(10, MLPClassifier(), data_80, labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#create a voting classifier with our 3 top models\n",
    "models = [('logr', LogisticRegression()), ('svc', SVC(kernel='linear')), ('mlp', MLPClassifier())]\n",
    "vote = VotingClassifier(estimators=models)\n",
    "kfold_test(10, vote, data_80, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have settled on our 3 best models and a good feature subset, it is time to tune the hyperparameters in the 3 models using GridSearch. This should hopefully increase our accuracy a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logr best score: 0.919885745375408\n",
      "{'fit_intercept': False, 'warm_start': True}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "logr = LogisticRegression()\n",
    "params = {'fit_intercept': (True, False), 'warm_start':(True, False)}\n",
    "search = GridSearchCV(logr, params, cv=5)\n",
    "search.fit(data_80, labels)\n",
    "\n",
    "print('logr best score: ' + str(search.best_score_))\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc best score: 0.919069640914037\n",
      "{'probability': True, 'shrinking': True}\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(kernel='linear')\n",
    "params = {'shrinking':(True, False), 'probability':(True, False)}\n",
    "search = GridSearchCV(svc, params, cv=5)\n",
    "search.fit(data_80, labels)\n",
    "\n",
    "print('svc best score: ' + str(search.best_score_))\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp best score: 0.9205658324265505\n",
      "{'activation': 'relu', 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier()\n",
    "params = {'activation':('identity', 'logistic', 'tanh', 'relu'), 'warm_start':(True, False)}\n",
    "search = GridSearchCV(mlp, params, cv=5)\n",
    "search.fit(data_80, labels)\n",
    "\n",
    "print('mlp best score: ' + str(search.best_score_))\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashwin/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/ashwin/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/ashwin/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/ashwin/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/ashwin/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/ashwin/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/ashwin/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/ashwin/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/ashwin/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/ashwin/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9258721532091098"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we rerun the voting classifier with the optimized parameters\n",
    "models = [('logr', LogisticRegression(fit_intercept=False, warm_start=True)), \n",
    "          ('svc', SVC(kernel='linear', probability=True, shrinking=True)), \n",
    "          ('mlp', MLPClassifier(activation='relu', warm_start=False))]\n",
    "vote = VotingClassifier(estimators=models)\n",
    "kfold_test(10, vote, data_80, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is the moment of truth: we train our model on the entire training dataset and assess its accuracy on the test dataset. The use of 3 very different algorithms in a voting ensemble should somewhat combat overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashwin/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9609772650152698"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vote = VotingClassifier(estimators=models)\n",
    "vote.fit(data, labels)\n",
    "vote.score(test, answers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
